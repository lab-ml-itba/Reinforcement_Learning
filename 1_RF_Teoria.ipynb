{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "referencias recomendadas:\n",
    "- https://www.youtube.com/watch?v=2pWv7GOvuf0&t=130s\n",
    "- https://keon.io/deep-q-learning/\n",
    "- https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "- https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagram.png](diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No hay supervisión, la realimentación está basada en una recompenza (**Reward**)\n",
    "- La realimentación puede estar atrasada en el tiempo (Instant reward, time delayed reward, **discount factor**)\n",
    "- **No** son secuencias I.I.D como en muchos modelos de aprendisaje supervisado (Secuencia correlacionada)\n",
    "- Un **Agente** que toma acciones y los resultados dependen directamente de ellas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mañobrar un Drone\n",
    "- Jugar al Backgammon\n",
    "- Administrar un portfolio de inversión\n",
    "- Lograr que un robot camine\n",
    "- Jugar juegos de Atari\n",
    "\n",
    "¿Cual es la recompenza en cada caso? ¿Esta atrasada en el tiempo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recompenza (Reward) $R_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es un escalar\n",
    "- Indica que tan bien el agente lo esta haciendo en el tiempo t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo: Tomar las acciones que maximizen la recompenza acumulada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las acciones pueden tener consecuencias a largo plazo\n",
    "- La recompenza puede estar retrasada en el tiempo\n",
    "- Puede ser importante rechazar recompenza inmediata para tener recompenzas mayores al largo plazo\n",
    "\n",
    "Ejemplo:\n",
    "- Una inversión financiera puede tardar meses en hacerse efectiva\n",
    "- En ajedrez bloquear al oponente puede ser una estrategia mejor que comer una pieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL-Modelo.png](RL-Modelo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historia: es la secuencia de observaciones, Recompenzas y Acciones hasta el momento actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATE: Es una función de la historia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge S_t = f(H_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es lo que se usa para determinar que será lo proximo que ocurrira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Agente tiene su propia representación del estado del entorno (environmient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Desision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge P[S_{t+1}|S_t] = P[S_{t+1}|S_1, S_2, ..., S_t]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que consideramos como estado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rat.png](rat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El agente construye su propia representación:\n",
    "- Historia completa\n",
    "- Un red recurrente\n",
    "- Combinadciones de los estados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes de un Agente en RL\n",
    "Nuestra meta es construir el agente que pueda tomar las decisiones\n",
    "\n",
    "- Policy: Como debe comportarse el Agente\n",
    "- Value Function: Que tan bueno es cada estado\n",
    "- Q Funcrion: Que tan bueno es cada estada con cada accion\n",
    "- Modelo: Representación del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deterministica:\n",
    "\n",
    "$\\huge a=\\pi(s)$\n",
    "\n",
    "Estocastica:\n",
    "\n",
    "$\\huge \\pi(a|s) = P[A_t=a|S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente usamos la deterministica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![policy.png](policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Function (Bellman Equation)\n",
    "- Predicción de la recompenza futura partiendo del estado **s**\n",
    "- Evalua que tan bueno es un estado\n",
    "- Permite tomar decisiones\n",
    "- **No** es lo mismo que la recompenza en un estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![value_func.png](value_func.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge v_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El subindice $\\pi$ indica que es para una policy particular\n",
    "\n",
    "La esperanza es debido al caracter aleatorio. Por ejemplo, cuando tomo una decisión, el proximo estado puede ser aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge v_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(s_{t+1})|S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge q_{\\pi}(s, a) = E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(s_{t+1}, a_{t+1})|S_t = s, A_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge v_{\\pi}(s) = \\sum_a \\pi(a|s)q_{\\pi}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo: encontrar la política $\\pi$ que maximize la Value Function Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si puedo encontrar Q luego solo tengo que hacer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge \\pi(s) = argmax_{a} q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "Metodo iterativo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promedio Ponderado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge q(s_t, a_t) \\leftarrow (1-\\alpha)q(s_t, a_t) + \\alpha (R_t +\\gamma max_a[q(s_{t+1},a)])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si el entorno (envornment) es totalmente deterministico entonces $\\alpha=1$ es optimo\n",
    "- Si $\\alpha=0$ el agente no aprende\n",
    "- $\\gamma = 1$ considera las recompensas futuras tan importantes como las inmediatos\n",
    "- $\\gamma = 0$ Solo le interesa la recompenza inmediata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimización del error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge \\hat{q}(s_t,a_t) = R +\\gamma max_a[q(s_{t+1},a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge e = \\hat q(s_t,a) - q(s_t,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizar error cuadratico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge q(s_t,a) = q(s_t,a)+\\alpha e$\n",
    "\n",
    "donde $\\alpha$ es el learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Misma promedio ponderado\n",
    "- Pero estima q(s,a) con una red\n",
    "\n",
    "Algoritmo:\n",
    "- Inicializo pesos de la red neuronal aleatoriamente\n",
    "- Ingreso con el estado inicial $s_0$\n",
    "- Me devuelve un listado de posibles acciones con sus pesos: $q(s_0, a)$\n",
    "- Elijo la accion mas probable a_0 (Con un grado de aleatoriedad que va disminuyendo con cada episodio) y guardo el valor $maxQ_0$ (valor de Q para esa accion)\n",
    "- Ejecuto la accion y tengo un nuevo estado $S_1$ con un posible reward $r_1$\n",
    "- Ingreso a la RNN con ese nuevo estado, obtengo $q(s_1,a)$ y me quedo con el valor mayor ($maxQ_1$)\n",
    "- Uso $maxQ_1$ para predecir el anterior $maxQ_0$: $r_1+\\gamma maxQ_1$\n",
    "- Es decir, le pido que haga un update de los pesos con entrada $s_0$ y salida $maxQ_0$ (Target, valor anotado)\n",
    "- Repito esto hasta que termina el juego (suponiendo que termina)\n",
    "- repito esto tantos episodios como sean necesarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based - Model Free\n",
    "Si es **model based**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intenta predecir lo como se comportará el entorno en cada paso\n",
    "- $P$ predice el próximo estado\n",
    "- $R$ predice la próxima recompenza inmediata (Inmediate reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge P_{SS^{\\prime}}^a = P[S_{t+1} = s^{\\prime}|S_t = s, A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge R_s^a = E[R_{t+1}|S_t=s, A_t = a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
